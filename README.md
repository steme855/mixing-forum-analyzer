# Mixing Forum Analyzer

[![Tests](https://github.com/steme855/mixing-forum-analyzer/actions/workflows/test.yml/badge.svg)](https://github.com/steme855/mixing-forum-analyzer/actions/workflows/test.yml)
[![codecov](https://codecov.io/gh/steme855/mixing-forum-analyzer/branch/main/graph/badge.svg)](https://codecov.io/gh/steme855/mixing-forum-analyzer)

> **Spart Audio Engineers 120â€¯h/Jahr** durch semantische Suche in Mixing-Foren. Findet wiederkehrende Probleme (Kick-Resonanzen, Vocal-Sibilance) in <1â€¯Sekunde.

## ğŸ’° Business Value

**Problem:** Audio Engineers verbringen 2â€“3â€¯h pro Woche mit der Recherche in Foren, um bekannte Mixing-Probleme erneut zu lÃ¶sen.

**LÃ¶sung:** Die semantische Suche findet Ã¤hnliche Cases sofort. SBERT/TF-IDF Matching + Preset-Recommender liefern direkte Handlungsempfehlungen.

**Impact:**
- Zeitersparnis: ~120â€¯h/Jahr â‰™ ca. 3.600â€¯â‚¬ bei 30â€¯â‚¬/h Freelance-Rate
- Wissensdatenbank: kuratierte Forenposts + Preset-Library fÃ¼r wiederkehrende Symptome
- Monitoring: Evaluation Notebook + Metriken-Dashboard fÃ¼r kontinuierliche Verbesserung

**Demo:** [Live-App (lokal via `streamlit run app.py`)] | **Impact-View:** Notebook-Export `docs/metrics_for_readme.md`

## ğŸ¥ Demo

![Demo](docs/demo.gif)

> Aufnahme-Idee: QuickTime/OBS, Query â€œKick klingt blechern, zu viel 3kHzâ€, Top-3 Ergebnisse zeigen, Preset-Empfehlung markieren.

## ğŸ“Š Performance

| Metrik | Wert | Benchmark |
|--------|------|-----------|
| MRR | 1.00 | >0.70 |
| Top-1-Accuracy | 1.00 | >0.60 |
| Top-3-Accuracy | 1.00 | >0.80 |
| Top-5-Accuracy | 1.00 | >0.90 |

Quelle: `notebooks/02_evaluation.ipynb` (29 Queries, TF-IDF Baseline). Markdown-Export siehe `docs/metrics_for_readme.md`.

## ğŸ” Evaluation Workflow

1. `notebooks/02_evaluation.ipynb` enthÃ¤lt 29 Test-Queries mit Ground-Truth-Mapping.
2. `evaluation/metrics.py` liefert MRR, Top-K Accuracy, Cosine-Verteilungen, Latenz-Stats.
3. Confusion-Analyse (False Positives/Negatives) wird als DataFrame erzeugt.
4. Ergebnisse kÃ¶nnen bei Bedarf in ein Dashboard integriert werden (`docs/metrics_for_readme.md`).

## ğŸ›ï¸ Preset Intelligence

- Symptom-Keywords â†’ Frequenzbereiche (z.â€¯B. â€œblechernâ€ â†’ Cut @ 3â€¯â€“â€¯5â€¯kHz)
- Severity-Scoring passt Gain-Empfehlungen an (light/medium/strong)
- Multi-Label Output: mehrere Presets pro Problem mÃ¶glich (`presets/preset_recommender.py`)

## âš™ï¸ Tech Stack

- Streamlit UI mit SBERT/TF-IDF Hybrid (Lazy Load, TF-IDF Fallback)
- spaCy DE (robuste Fallback-Logik, Diagnostics in der App)
- RapidFuzz fÃ¼r Keyword-Boosting & Synonym-Handling
- Evaluation: Pandas, NumPy, eigene Metrics-Library (`evaluation/metrics.py`)

## ğŸš€ Quickstart (5 Minuten)

```bash
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
python -m spacy download de_core_news_sm
streamlit run app.py
```

## ğŸ—‚ï¸ Daten-Pipeline

Siehe `data/README.md` fÃ¼r Quellen, Preprocessing und Lizenzhinweise. Rohdaten â†’ `data/raw/`, bereinigte Texte â†’ `data/processed/`, Embeddings â†’ `data/embeddings/`.

## ğŸ§ª Tests & CI

- Unit-, Integration- und Smoke-Tests (`tests/`) decken Search, Presets und Streamlit-Import ab (15+ Checks).
- Test-Workflow (`.github/workflows/test.yml`): Python 3.9/3.10/3.11 Matrix, flake8, black, isort, Pytest mit Coverage, Upload zu Codecov.
- Deploy-Workflow (`.github/workflows/deploy.yml`): Automatisches HuggingFace-Deploy bei Ã„nderungen an `app/`, `requirements.txt` oder README inkl. Health-Check.
- `pytest.ini`, `pyproject.toml`, `Makefile`, `requirements-dev.txt` bÃ¼ndeln Enterprise-Readiness (Linting, Formatting, Docker, TDD-Loop).
- Setup-Guide mit Secret-Handling & Badges: `docs/setup_guide.md`.
