{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ffe70e-32ae-4e29-ac3c-e0c7ba3f2fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sm/Desktop/mixing-forum-analyzer/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)  # sollte auf .../mixing-forum-analyzer/venv/... zeigen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc77960f-2501-4ed1-970f-ac545588b30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook & JupyterLab sind in der venv verfügbar ✅\n"
     ]
    }
   ],
   "source": [
    "import notebook, jupyterlab\n",
    "print(\"Notebook & JupyterLab sind in der venv verfügbar ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e791f22-9679-4ffe-b84d-e547e4b41b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea47216f-8612-4c83-a0cc-7a357ba28f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH = pathlib.Path(\"../data/sample_corpus.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a94b4e5-2a50-4bee-a501-9e3fc4ab2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = json.loads(CORPUS_PATH.read_text(encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a25b14e-acdd-4ae7-91c3-0aa016376475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " ['Die Kickdrum pumpt im Mix, aber die Snare wirkt zu dünn.',\n",
       "  'Vocals sitzen zu weit hinten, mehr Präsenz im 3 kHz Bereich.',\n",
       "  'Die Snare klingt trocken und etwas hart, vielleicht mehr Raumanteil.'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus), corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a35613-8e4c-419a-9a81-af4556db0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "885d6927-2256-4eff-9a61-38ec799b15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokens(s: str):\n",
    "    return re.findall(r\"\\w+\", s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e05a63d1-3618-4cca-a595-8a2b973c1655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['die', 'kickdrum', 'pumpt', 'im', 'mix', 'aber', 'die', 'snare', 'wirkt', 'zu', 'dünn']\n",
      "2 ['vocals', 'sitzen', 'zu', 'weit', 'hinten', 'mehr', 'präsenz', 'im', '3', 'khz', 'bereich']\n",
      "3 ['die', 'snare', 'klingt', 'trocken', 'und', 'etwas', 'hart', 'vielleicht', 'mehr', 'raumanteil']\n",
      "4 ['bassdrum', 'und', 'kickdrum', 'werden', 'oft', 'verwechselt', 'ich', 'brauche', 'mehr', 'punch']\n",
      "5 ['die', 'hi', 'hats', 'sind', 'zu', 'scharf', 'ein', 'sanfter', 'low', 'pass', 'könnte', 'helfen']\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(corpus[:5], 1):\n",
    "    print(i, simple_tokens(s))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f7512ee-40a2-404d-b2e4-715372c26d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b747cd7d-f743-4d9a-b450-0b65f5f34b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1352e8da-5ff2-495f-828e-96117b3cb18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02ffcf37-3981-4e24-91a5-234ff1552917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn Version: 1.5.1\n"
     ]
    }
   ],
   "source": [
    "print(\"scikit-learn Version:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4574772d-bace-4d79-ac96-3b463505dfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas Version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(\"pandas Version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb0798-42bd-4388-a4ad-e9051bcfba9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "779a8c73-14fd-433b-b7ea-ae00d5559c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f2f9763-fa7e-4471-9c44-01355eb18698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ff0fef2-3d2c-4447-8621-58095d284e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a332da-35f9-4852-84c4-c9dc965445de",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'ihr', 'das', 'zu', 'aber', 'für', 'und', 'du', 'mit', 'die', 'von', 'ein', 'dem', 'mehr', 'eine', 'ich', 'oder', 'den', 'man', 'in', 'wir', 'im', 'am', 'einer', 'der'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m german_stop = {\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mder\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdie\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdas\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mund\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33moder\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33maber\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mim\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33min\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mam\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mein\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33meine\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33meiner\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mzu\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmit\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mvon\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mfür\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmehr\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mich\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdu\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mwir\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mihr\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mman\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mden\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdem\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m }\n\u001b[32m      5\u001b[39m tfidf = TfidfVectorizer(lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m, stop_words=german_stop)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X_tfidf = \u001b[43mtfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out()).head(\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mixing-forum-analyzer/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2091\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2084\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2085\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2086\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2087\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2088\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2089\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2090\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2091\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2093\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2094\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mixing-forum-analyzer/venv/lib/python3.12/site-packages/sklearn/base.py:1466\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1461\u001b[39m partial_fit_and_fitted = (\n\u001b[32m   1462\u001b[39m     fit_method.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mpartial_fit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[32m   1463\u001b[39m )\n\u001b[32m   1465\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[32m-> \u001b[39m\u001b[32m1466\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m   1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mixing-forum-analyzer/venv/lib/python3.12/site-packages/sklearn/base.py:666\u001b[39m, in \u001b[36mBaseEstimator._validate_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    659\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[32m    660\u001b[39m \n\u001b[32m    661\u001b[39m \u001b[33;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    664\u001b[39m \u001b[33;03m    accepted constraints.\u001b[39;00m\n\u001b[32m    665\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mixing-forum-analyzer/venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:95\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     90\u001b[39m     constraints_str = (\n\u001b[32m     91\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     92\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     93\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     96\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     98\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'ihr', 'das', 'zu', 'aber', 'für', 'und', 'du', 'mit', 'die', 'von', 'ein', 'dem', 'mehr', 'eine', 'ich', 'oder', 'den', 'man', 'in', 'wir', 'im', 'am', 'einer', 'der'} instead."
     ]
    }
   ],
   "source": [
    "german_stop = {\n",
    "    \"der\",\"die\",\"das\",\"und\",\"oder\",\"aber\",\"im\",\"in\",\"am\",\"ein\",\"eine\",\"einer\",\n",
    "    \"zu\",\"mit\",\"von\",\"für\",\"mehr\",\"ich\",\"du\",\"wir\",\"ihr\",\"man\",\"den\",\"dem\"\n",
    "}\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=german_stop)\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out()).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0858aac2-f28a-47c2-a596-1f567b16140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_stop = {\n",
    "    \"der\",\"die\",\"das\",\"und\",\"oder\",\"aber\",\"im\",\"in\",\"am\",\"ein\",\"eine\",\"einer\",\n",
    "    \"zu\",\"mit\",\"von\",\"für\",\"mehr\",\"ich\",\"du\",\"wir\",\"ihr\",\"man\",\"den\",\"dem\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54017bd6-5fb8-4895-932e-48f8b7a36ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=german_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b66c01c-ff42-4a0c-b2de-8829fa16050d",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'ihr', 'das', 'zu', 'aber', 'für', 'und', 'du', 'mit', 'die', 'von', 'ein', 'dem', 'mehr', 'eine', 'ich', 'oder', 'den', 'man', 'in', 'wir', 'im', 'am', 'einer', 'der'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_tfidf = \u001b[43mtfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mixing-forum-analyzer/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2091\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2084\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2085\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2086\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2087\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2088\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2089\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2090\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2091\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2093\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2094\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mixing-forum-analyzer/venv/lib/python3.12/site-packages/sklearn/base.py:1466\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1461\u001b[39m partial_fit_and_fitted = (\n\u001b[32m   1462\u001b[39m     fit_method.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mpartial_fit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[32m   1463\u001b[39m )\n\u001b[32m   1465\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[32m-> \u001b[39m\u001b[32m1466\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m   1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mixing-forum-analyzer/venv/lib/python3.12/site-packages/sklearn/base.py:666\u001b[39m, in \u001b[36mBaseEstimator._validate_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    659\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[32m    660\u001b[39m \n\u001b[32m    661\u001b[39m \u001b[33;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    664\u001b[39m \u001b[33;03m    accepted constraints.\u001b[39;00m\n\u001b[32m    665\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mixing-forum-analyzer/venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:95\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     90\u001b[39m     constraints_str = (\n\u001b[32m     91\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     92\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     93\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     96\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     98\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'ihr', 'das', 'zu', 'aber', 'für', 'und', 'du', 'mit', 'die', 'von', 'ein', 'dem', 'mehr', 'eine', 'ich', 'oder', 'den', 'man', 'in', 'wir', 'im', 'am', 'einer', 'der'} instead."
     ]
    }
   ],
   "source": [
    "X_tfidf = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "367b3fb0-c094-4185-912a-7b048dd39af1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pd.DataFrame(\u001b[43mX_tfidf\u001b[49m.toarray(), columns=tfidf.get_feature_names_out()).head(\u001b[32m3\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out()).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62365f32-d50a-4ef3-8237-593194138e4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m german_stop = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mder\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdie\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdas\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mund\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33moder\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33maber\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mim\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33min\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mam\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mein\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33meine\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33meiner\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mzu\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmit\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mvon\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mfür\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmehr\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mich\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdu\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mwir\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mihr\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mman\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mden\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdem\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tfidf = \u001b[43mTfidfVectorizer\u001b[49m(lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m, stop_words=german_stop)\n\u001b[32m      6\u001b[39m X_tfidf = tfidf.fit_transform(corpus)\n",
      "\u001b[31mNameError\u001b[39m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "german_stop = [\n",
    "    \"der\",\"die\",\"das\",\"und\",\"oder\",\"aber\",\"im\",\"in\",\"am\",\"ein\",\"eine\",\"einer\",\n",
    "    \"zu\",\"mit\",\"von\",\"für\",\"mehr\",\"ich\",\"du\",\"wir\",\"ihr\",\"man\",\"den\",\"dem\"\n",
    "]\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=german_stop)\n",
    "X_tfidf = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d8eef0-decb-46ee-bbb1-d81d0acd7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9993b64c-9866-4a68-8dde-88179dfbb2b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m german_stop = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mder\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdie\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdas\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mund\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33moder\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33maber\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mim\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33min\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mam\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mein\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33meine\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33meiner\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mzu\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmit\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mvon\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mfür\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmehr\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mich\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdu\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mwir\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mihr\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mman\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mden\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdem\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m ]\n\u001b[32m      5\u001b[39m tfidf = TfidfVectorizer(lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m, stop_words=german_stop)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X_tfidf = tfidf.fit_transform(\u001b[43mcorpus\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "german_stop = [\n",
    "    \"der\",\"die\",\"das\",\"und\",\"oder\",\"aber\",\"im\",\"in\",\"am\",\"ein\",\"eine\",\"einer\",\n",
    "    \"zu\",\"mit\",\"von\",\"für\",\"mehr\",\"ich\",\"du\",\"wir\",\"ihr\",\"man\",\"den\",\"dem\"\n",
    "]\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=german_stop)\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce35de0e-ad65-436f-8d7e-1a77b5a62e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba8686f-b7f0-4346-8322-8372633ea5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_stop = [\n",
    "    \"der\",\"die\",\"das\",\"und\",\"oder\",\"aber\",\"im\",\"in\",\"am\",\"ein\",\"eine\",\"einer\",\n",
    "    \"zu\",\"mit\",\"von\",\"für\",\"mehr\",\"ich\",\"du\",\"wir\",\"ihr\",\"man\",\"den\",\"dem\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "067d9a93-d3fc-4ea2-8c1a-cac00f0b145b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tfidf = \u001b[43mTfidfVectorizer\u001b[49m(lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m, stop_words=german_stop)\n",
      "\u001b[31mNameError\u001b[39m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=german_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07373e9-a004-4274-a4d0-cc0cc387db06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_tfidf = \u001b[43mtfidf\u001b[49m.fit_transform(corpus)\n",
      "\u001b[31mNameError\u001b[39m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "X_tfidf = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f043b690-c0af-487f-97a0-e0202a8c22a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tfidf = \u001b[43mTfidfVectorizer\u001b[49m(lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m, stop_words=spacy_stop) \n",
      "\u001b[31mNameError\u001b[39m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=spacy_stop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9cb299-a6f8-42de-9c68-02485fa07794",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tfidf = \u001b[43mTfidfVectorizer\u001b[49m(lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m, stop_words=spacy_stop) \n",
      "\u001b[31mNameError\u001b[39m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=spacy_stop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bba16-fd9d-4c40-8593-52888069c7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8824232-e523-4781-8093-e3ad0d7ceb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4863c0ad-d486-4406-b133-451138d57aa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pd.DataFrame(\u001b[43mX_tfidf\u001b[49m.toarray(), columns=tfidf.get_feature_names_out()).head(\u001b[32m3\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out()).head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0d632c-0f0f-4277-860d-1e10ecde8cd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pd.DataFrame(\u001b[43mX_tfidf\u001b[49m.toarray(), columns=tfidf.get_feature_names_out()).head(\u001b[32m3\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out()).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f16ead-91d3-49fb-a1d3-76bf49032af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumente im Corpus: 10\n",
      "Hinweis: spaCy DE Modell nicht geladen – Fallback-Stopwörter aktiv.\n",
      "TF-IDF Matrix: (10, 139)\n",
      "Top-Wörter für Doc 0: [('pumpt mix', 0.312), ('snare wirkt', 0.312), ('wirkt dünn', 0.312), ('wirkt', 0.312), ('kickdrum pumpt', 0.312)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ähnlichkeit</th>\n",
       "      <th>Post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.294</td>\n",
       "      <td>Snare zu boxig, 300–500 Hz absenken, transient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.120</td>\n",
       "      <td>Mehr Glue auf dem Drum-Bus, Kleber-Kompressor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119</td>\n",
       "      <td>Bassdrum und Kickdrum werden oft verwechselt –...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.103</td>\n",
       "      <td>Sub-Bass ist maskierend, Sidechain von Kick zu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073</td>\n",
       "      <td>Die Kickdrum pumpt im Mix, aber die Snare wirk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ähnlichkeit                                               Post\n",
       "0        0.294  Snare zu boxig, 300–500 Hz absenken, transient...\n",
       "1        0.120  Mehr Glue auf dem Drum-Bus, Kleber-Kompressor ...\n",
       "2        0.119  Bassdrum und Kickdrum werden oft verwechselt –...\n",
       "3        0.103  Sub-Bass ist maskierend, Sidechain von Kick zu...\n",
       "4        0.073  Die Kickdrum pumpt im Mix, aber die Snare wirk..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Imports & Setup ---\n",
    "import json, pathlib, re, numpy as np, pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "\n",
    "# --- Corpus laden ---\n",
    "CORPUS_PATH = pathlib.Path(\"../data/sample_corpus.json\")\n",
    "corpus = json.loads(CORPUS_PATH.read_text(encoding=\"utf-8\"))\n",
    "print(f\"Dokumente im Corpus: {len(corpus)}\")\n",
    "\n",
    "# --- Stopwörter: spaCy (mit Fallback) ---\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    stop_words = list(nlp.Defaults.stop_words)  # Liste, nicht Set!\n",
    "except Exception as e:\n",
    "    # Fallback, falls spaCy-Modell lokal nicht verfügbar\n",
    "    stop_words = [\n",
    "        \"der\",\"die\",\"das\",\"und\",\"oder\",\"aber\",\"im\",\"in\",\"am\",\"ein\",\"eine\",\"einer\",\n",
    "        \"zu\",\"mit\",\"von\",\"für\",\"mehr\",\"ich\",\"du\",\"wir\",\"ihr\",\"man\",\"den\",\"dem\",\"einmal\"\n",
    "    ]\n",
    "    print(\"Hinweis: spaCy DE Modell nicht geladen – Fallback-Stopwörter aktiv.\")\n",
    "\n",
    "# --- TF-IDF (inkl. 2-gramme für Phrasen wie 'zu scharf') ---\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=stop_words, ngram_range=(1,2))\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "features = np.array(tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\", X_tfidf.shape)\n",
    "\n",
    "# --- Helper: Top-Terme je Dokument ---\n",
    "def top_terms(row_vec, k=5):\n",
    "    arr = row_vec.toarray().ravel()\n",
    "    idx = arr.argsort()[::-1][:k]\n",
    "    return list(zip(features[idx], arr[idx].round(3)))\n",
    "\n",
    "# --- Similarity-Funktion ---\n",
    "def most_similar(query: str, topk: int = 5):\n",
    "    qv = tfidf.transform([query])\n",
    "    sims = cosine_similarity(qv, X_tfidf).ravel()\n",
    "    idx = sims.argsort()[::-1][:topk]\n",
    "    return pd.DataFrame({\n",
    "        \"Ähnlichkeit\": sims[idx].round(3),\n",
    "        \"Post\": [corpus[i] for i in idx]\n",
    "    })\n",
    "\n",
    "# --- Mini-Demo ---\n",
    "print(\"Top-Wörter für Doc 0:\", top_terms(X_tfidf[0], k=5))\n",
    "most_similar(\"Snare zu boxig, mehr Punch auf Kick\", topk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3fbc8e0-b174-4c76-b616-00d8ce8a9e92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[32m     10\u001b[39m tfidf = TfidfVectorizer(analyzer=spacy_lemma_analyzer, ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m X_tfidf = tfidf.fit_transform(\u001b[43mcorpus\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def spacy_lemma_analyzer(doc):\n",
    "    # nur Lemmas, keine Stoppwörter/Interpunktion/Ziffern\n",
    "    return [t.lemma_.lower() for t in nlp(doc)\n",
    "            if not (t.is_stop or t.is_punct or t.like_num)]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(analyzer=spacy_lemma_analyzer, ngram_range=(1,2))\n",
    "X_tfidf = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22acfbd8-37dc-4e76-89f1-51f58796bc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
