{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-03",
   "metadata": {},
   "source": [
    "# 03 — Cosine Similarity (TF‑IDF & optional SBERT)\n",
    "\n",
    "Ziel: Ähnlichkeit zwischen kurzen Mixing-Posts berechnen und Top‑Treffer anzeigen.\n",
    "\n",
    "**Was du lernst**\n",
    "- TF‑IDF Vektorisierung und Cosine Similarity\n",
    "- Optional: Semantische Embeddings (SBERT) als Vergleich\n",
    "- Einfache Query‑Funktionen und Mini‑Eval (sanity check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup & Daten laden\n",
    "Wir laden den Korpus aus `data/sample_corpus.json`. Falls die Datei fehlt, nutzen wir einen kleinen Fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel  # Cosine for L2-normalized vectors\n",
    "\n",
    "DATA = Path(\"data\"); DATA.mkdir(exist_ok=True)\n",
    "\n",
    "def load_corpus():\n",
    "    p = DATA/\"sample_corpus.json\"\n",
    "    if p.exists():\n",
    "        with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            x = json.load(f)\n",
    "            if isinstance(x, list) and all(isinstance(t, str) for t in x):\n",
    "                return x\n",
    "    # Fallback (Woche 1 Demo)\n",
    "    return [\n",
    "        \"Die Snare ist zu laut und harsch\",\n",
    "        \"Kick zu weich, es fehlt der Punch\",\n",
    "        \"Vocals klingen nasal, 800 Hz absenken\",\n",
    "        \"Bass maskiert die Kick, Sidechain nötig\",\n",
    "        \"S-Laute sind scharf, De-Esser einsetzen\",\n",
    "    ]\n",
    "\n",
    "corpus = load_corpus()\n",
    "len(corpus), corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tfidf-build",
   "metadata": {},
   "source": [
    "## TF‑IDF aufbauen\n",
    "Wir nutzen Unigramme und Bigramme. `linear_kernel` liefert bei normalisierten TF‑IDF‑Vektoren die **Cosine Similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tfidf-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, ngram_range=(1,2), min_df=1)\n",
    "X = tfidf.fit_transform(corpus)\n",
    "X.shape, len(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tfidf-rank",
   "metadata": {},
   "source": [
    "## Ähnlichkeit berechnen (TF‑IDF)\n",
    "Die Funktion `rank_tfidf` gibt sortierte Dokumentindizes sowie die zugehörigen Similarity‑Scores zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tfidf-rank-fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_tfidf(query: str, k: int = 5):\n",
    "    qv = tfidf.transform([query])\n",
    "    sims = linear_kernel(qv, X).ravel()  # entspricht Cosine, da TF‑IDF normiert\n",
    "    order = np.argsort(-sims)\n",
    "    topk = order[:k]\n",
    "    return topk.tolist(), sims[topk].tolist()\n",
    "\n",
    "# Sanity‑Check\n",
    "idxs, scores = rank_tfidf(\"snare zu laut\", k=min(5, len(corpus)))\n",
    "pd.DataFrame({\"rank\": np.arange(1, len(idxs)+1), \"doc\": [corpus[i] for i in idxs], \"score\": np.round(scores, 3)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sbert-optional",
   "metadata": {},
   "source": [
    "## Optional: SBERT (semantisch, CPU)\n",
    "Falls `sentence_transformers` und `torch` verfügbar sind, vergleichen wir die Resultate. Bei Triton‑Konflikten (macOS/CPU) deinstalliere `triton` und pinne Torch (z. B. `torch==2.2.2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sbert-try",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_sbert(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            model = SentenceTransformer(model_name, device=\"cpu\")\n",
    "        doc_emb = model.encode(corpus, normalize_embeddings=True)\n",
    "        def rank(query: str, k: int = 5):\n",
    "            qv = model.encode([query], normalize_embeddings=True)\n",
    "            sims = (qv @ doc_emb.T).ravel()\n",
    "            order = np.argsort(-sims)\n",
    "            topk = order[:k]\n",
    "            return topk.tolist(), sims[topk].tolist()\n",
    "        return rank\n",
    "    except Exception as e:\n",
    "        print(\"SBERT nicht verfügbar:\", e)\n",
    "        return None\n",
    "\n",
    "rank_sbert = try_sbert()\n",
    "rank_sbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare",
   "metadata": {},
   "source": [
    "## Vergleich: TF‑IDF vs. SBERT (für eine Query)\n",
    "Wir vergleichen die Top‑Treffer der beiden Methoden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"kick zu weich\"\n",
    "k = min(5, len(corpus))\n",
    "\n",
    "tf_idx, tf_sc = rank_tfidf(query, k)\n",
    "tf_df = pd.DataFrame({\n",
    "    \"rank\": np.arange(1, len(tf_idx)+1),\n",
    "    \"method\": \"tfidf\",\n",
    "    \"doc\": [corpus[i] for i in tf_idx],\n",
    "    \"score\": np.round(tf_sc, 3)\n",
    "})\n",
    "\n",
    "if rank_sbert:\n",
    "    sb_idx, sb_sc = rank_sbert(query, k)\n",
    "    sb_df = pd.DataFrame({\n",
    "        \"rank\": np.arange(1, len(sb_idx)+1),\n",
    "        \"method\": \"sbert\",\n",
    "        \"doc\": [corpus[i] for i in sb_idx],\n",
    "        \"score\": np.round(sb_sc, 3)\n",
    "    })\n",
    "    out = pd.concat([tf_df, sb_df], ignore_index=True)\n",
    "else:\n",
    "    out = tf_df\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini-eval",
   "metadata": {},
   "source": [
    "## Mini‑Eval (sanity check)\n",
    "Kleines Ground‑Truth Mapping: Query → relevante Dokument‑Indizes. Wir berechnen **MRR** und **Precision@k** (k=3) für TF‑IDF (und SBERT, falls verfügbar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT = {\n",
    "    \"snare zu laut\": [0],\n",
    "    \"kick zu weich\": [1],\n",
    "    \"vocals nasal 800 hz\": [2],\n",
    "    \"bass maskiert kick sidechain\": [3],\n",
    "    \"s-laute scharf de-esser\": [4],\n",
    "}\n",
    "\n",
    "def precision_at_k(relevants, retrieved, k=3):\n",
    "    R = set(relevants); topk = retrieved[:k]\n",
    "    hits = sum(1 for i in topk if i in R)\n",
    "    return hits / max(1, len(topk))\n",
    "\n",
    "def reciprocal_rank(relevants, retrieved):\n",
    "    R = set(relevants)\n",
    "    for r, idx in enumerate(retrieved, 1):\n",
    "        if idx in R:\n",
    "            return 1.0 / r\n",
    "    return 0.0\n",
    "\n",
    "def evaluate(run_name, rank_fn):\n",
    "    rows = []\n",
    "    for q, rel in GT.items():\n",
    "        idxs, _ = rank_fn(q, k=min(5, len(corpus)))\n",
    "        rows.append({\n",
    "            \"run\": run_name,\n",
    "            \"query\": q,\n",
    "            \"MRR\": reciprocal_rank(rel, idxs),\n",
    "            \"P@3\": precision_at_k(rel, idxs, 3)\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    agg = df.drop(columns=[\"query\"]).groupby(\"run\").mean(numeric_only=True).round(3)\n",
    "    return df, agg\n",
    "\n",
    "tf_df, tf_agg = evaluate(\"tfidf\", rank_tfidf)\n",
    "display(tf_agg); display(tf_df)\n",
    "\n",
    "if rank_sbert:\n",
    "    sb_df, sb_agg = evaluate(\"sbert\", rank_sbert)\n",
    "    display(sb_agg); display(sb_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## Übungen (kurz)\n",
    "1. Erweitere den Korpus (10–20 Zeilen) und beobachte, wie sich die Rankings ändern.\n",
    "2. Ändere die `ngram_range` auf `(1,3)` und vergleiche.\n",
    "3. Füge einfache Synonyme/Query‑Expansion hinzu (z. B. *kick* → *bassdrum*), bevor du `rank_tfidf` aufrufst.\n",
    "4. (Optional) Evaluiere verschiedene TF‑IDF‑Parameter (z. B. Stopwörter, `min_df`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0778b8-9bc0-4974-b942-c70eb4a5dba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef1087-072c-495c-97f4-bfb0f57b15b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af83bf0-9507-4167-aa70-9a11a51bd96e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
